# -*- coding: utf-8 -*-
"""
Python File Template 
"""

import os

import h5py
import numpy as np
import pandas as pd
import pickle
import json

import xml.etree.ElementTree

import re

import sys

__author__ = "Rui Meng"
__email__ = "rui.meng@pitt.edu"

ROOT_PATH = "/Users/memray/Project/upmc_wsd/wsd_data/sense_inventory/"

# A very complete UMLS thesaurus providing sense unique id, many longform variants and a few abbreviations
UMLS_sense_file_path = os.path.join(ROOT_PATH, 'raw', 'MRCONSO.RRF')
# A sense inventory generated by ADAM method based on MEDLINE paper data, containing a lot of noise
MEDLINE_sense_xml_path = os.path.join(ROOT_PATH, 'raw', 'normalized_abbr_sense2.xml')

# A sense inventory by merging both UMLS and MEDLINE
UMLS_OR_MEDLINE_sense_inventory_path = os.path.join(ROOT_PATH, 'umls_or_medline_sense_inventory.pkl')
# A filtered UMLS sense inventory by only retaining senses appearing in MEDLINE
UMLS_AND_MEDLINE_sense_inventory_path = os.path.join(ROOT_PATH, 'umls_and_medline_sense_inventory.pkl')

# A cleaned sense inventory used for further filtering.
# provided by Zhendong used for annotation, for each sense only one abbr and one longform are provided
CLEANED_SENSE_inventory_csv_path = os.path.join(ROOT_PATH, 'sense_inventory_cleaned_annotation.csv')

# output file path
FINAL_CLEANED_SENSE_INVENTORY_PICKLE_PATH = os.path.join(ROOT_PATH, 'final_cleaned_sense_inventory_with_testsets.pkl')
FINAL_CLEANED_SENSE_INVENTORY_JSON_PATH = os.path.join(ROOT_PATH, 'final_cleaned_sense_inventory_with_testsets.json')

# common words dict path
GOOGLE_20K_COMMON_WORDS_VOCAB_PATH = os.path.join(ROOT_PATH, 'google-10000-english', '20k.txt')

# annotated by Rebecca, to untangle abbr-longform mappings
LONGFORM_ABBR_ANNOTATED_MAPPING_PATH = os.path.join(ROOT_PATH, 'longForm_abbr_map.csv')

TEST_DATASET_BAES_PATH = os.path.join(ROOT_PATH, 'testset')

ABBR_WHITELIST = {'CPT 1', '4 ASA', 'HSAN 3', 'HSAN III',
                  'Epi DX', 'MEN 2B', 'CRPS I', 'DIN 1B',
                  'EC Tab', 'Ad Lib', 'EC Cell', 'CHF NOS',
                  'MEN IIB', 'Q fever', 'Vag Tab', 'AS ODNs',
                  'MPS I H', 'IV DRIP', 'Burn NOS', 'PEG 8O00',
                  'Chew Tab', 'TAB CHEW', 'MOAB LL2', 'Vag Supp',
                  '4\'-epi DX', 'MoAb CD52', 'MOAB HER2', 'MoAb VEGF',
                  'TAB EFFRV', 'DEXA scan', 'rHuIFN-a 2a', 'Tc-99m MIBI',
                  'rhuMAb HER2', 'dThdPase', 'RARalpha', 'RARgamma'}

CUI_BLACKLIST = {'C0000925', 'C0001861', 'C0002957', 'C0003761', 'C0004309', 'C0004381', 'C0004896', 'C0004916',
                 'C0004950', 'C1508746', 'C1444754', 'C1305399', 'C1301886', 'C1301808', 'C1299586', 'C1298908',
                 'C1282911', 'C1282910', 'C1280519', 'C1280412', 'C1279941', 'C0947630', 'C0939937', 'C0939895',
                 'C0939869', 'C0936050', 'C0936040', 'C0936039', 'C0934502', 'C0817662', 'C0814225', 'C0806909',
                 'C0750572', 'C0750484', 'C0747726', 'C0681842', 'C0679105', 'C0679138', 'C0678946', 'C0678594'}

UPPER_LONGFORM_TO_REMOVE = {'W', 'P', 'B', 'I', 'ADD', }
UPPER_LONGFORM_TO_ABBR = {'W', 'P', 'B', 'I'}

class Sense():
    def __init__(self, cui):
        self.cui = cui
        self.in_testset = False
        self.medline_id_set = set()
        self.common_name_set = set()
        self.abbr_set = set()
        self.longform_set = set()
        self.tty_list = []
        self.text_list = []
        self.original_umls_dicts = []
        self.original_medline_dicts = []
        self.longform_abbr_mappings = []

    def add_medline_id(self, medline_id):
        self.medline_id_set.add(medline_id)

    def add_common_name(self, common_name):
        self.common_name_set.add(common_name)

    def add_abbr(self, abbr):
        self.abbr_set.add(abbr)

    def add_longform(self, longform):
        self.longform_set.add(longform)

    def add_tty(self, tty):
        self.tty_list.append(tty)

    def add_text(self, text):
        self.text_list.append(text)

    def __str__(self):
        retune_str = 'CUI: %s\n' % (self.cui)
        retune_str += 'common_name [num=%d]: %s\n' % (len(self.common_name_set), self.common_name_set)
        retune_str += 'ABBR [num=%d]: %s\n' % (len(self.abbr_set), self.abbr_set)
        retune_str += 'LONGFORM [num=%d]: %s' % (len(self.longform_set), self.longform_set)
        if hasattr(self, 'LONGFORM_ABBR_MAPPINGS'):
            retune_str += 'LONGFORM_ABBR_MAPPINGS [num=%d]: %s' % (len(self.LONGFORM_ABBR_MAPPINGS), self.LONGFORM_ABBR_MAPPINGS)

        return retune_str

    def to_dict(self):
        sense_dict = {'CUI': self.cui,
                      'IN_TESTSET': self.in_testset,
                      'ABBR': list(self.abbr_set),
                      'LONGFORM': list(self.longform_set),
                      'COMMON_NAME': list(self.common_name_set),
                      'UMLS': self.original_umls_dicts,
                      'MEDLINE_ID': list(self.medline_id_set),
                      'MEDLINE': self.original_medline_dicts,
                      'LONGFORM_ABBR_MAPPINGS': self.longform_abbr_mappings
                                                if hasattr(self, 'longform_abbr_mappings')
                                                else None
        }

        return sense_dict


def save_as_pickled_object(obj, filepath):
    """
    This is a defensive way to write pickle.write, allowing for very large files on all platforms
    """
    max_bytes = 2**31 - 1
    bytes_out = pickle.dumps(obj)
    n_bytes = sys.getsizeof(bytes_out)
    with open(filepath, 'wb') as f_out:
        for idx in range(0, n_bytes, max_bytes):
            f_out.write(bytes_out[idx:idx+max_bytes])


def try_to_load_as_pickled_object_or_None(filepath):
    """
    This is a defensive way to write pickle.load, allowing for very large files on all platforms
    """
    max_bytes = 2**31 - 1
    try:
        input_size = os.path.getsize(filepath)
        bytes_in = bytearray(0)
        with open(filepath, 'rb') as f_in:
            for _ in range(0, input_size, max_bytes):
                bytes_in += f_in.read(max_bytes)
        obj = pickle.loads(bytes_in)
    except:
        return None
    return obj


def load_umls_senses():
    if os.path.exists(UMLS_sense_file_path + '.cache'):
        umls_sense_inventory_dict, longform_cui_dict = try_to_load_as_pickled_object_or_None(UMLS_sense_file_path + '.cache')
    else:
        umls_sense_inventory_dict = {}
        longform_cui_dict = {}

        umls_names = ["CUI", "LAT", "TS", "LUI", "STT", "SUI", "ISPREF", "AUI", "SAUI", "SCUI", "SDUI", "SAB", "TTY", "CODE", "STR", "SRL", "SUPPRESS", "256"]
        umls_df = pd.read_csv(UMLS_sense_file_path, sep='|', header=None, names=umls_names, index_col=False)

        grouped_umls_df = umls_df.groupby('CUI')

        for group_id, (cui, group) in enumerate(grouped_umls_df):
            if group_id > 0 and group_id % 1000 == 0:
                print('group #%d' % group_id)
                # break

            sense = Sense(cui = cui)
            sense.original_umls_dicts = [r[1].to_dict() for r in list(group.iterrows())]
            # ingore non-English items
            sense.original_umls_dicts = [r for r in sense.original_umls_dicts if r['LAT'] == 'ENG']

            # for row_id, row in group.iterrows():
            #     cui = row['CUI']
            #     tty = str(row['TTY'])
            #     text = row['STR']
            #
            #     try:
            #         if tty.endswith('AB'):
            #             sense.add_abbr(text)
            #         else:
            #             sense.add_longform(text)
            #
            #     except Exception as e:
            #         print(cui)
            #         print(tty)
            #         print(text)
            #         print(str(e)+'\n')

            umls_sense_inventory_dict[cui] = sense
            for umls_record in sense.original_umls_dicts:
                cui_set = longform_cui_dict.get(umls_record['STR'], set())
                cui_set.add(umls_record['CUI'])
                longform_cui_dict[umls_record['STR']] = cui_set

        print('Loaded %d CUI from UMLS' % len(umls_sense_inventory_dict))
        print('Loaded %d longforms from UMLS' % len(longform_cui_dict))

        save_as_pickled_object([umls_sense_inventory_dict, longform_cui_dict], UMLS_sense_file_path + '.cache')


    return umls_sense_inventory_dict, longform_cui_dict


def deprecated_load_umls_senses():
    umls_names = ["CUI", "LAT", "TS", "LUI", "STT", "SUI", "ISPREF", "AUI", "SAUI", "SCUI", "SDUI", "SAB", "TTY", "CODE", "STR", "SRL", "SUPPRESS", "256"]
    umls_df = pd.read_csv(UMLS_sense_file_path, sep='|', header=None, names=umls_names, index_col=False)

    umls_sense_dict = {}

    for chunk in umls_df:
        for _, row in chunk.iterrows():
            cui = row['CUI']
            tty = str(row['TTY'])
            text = row['STR']

            if cui not in umls_sense_dict:
                sense = Sense(cui = cui)
            else:
                sense = umls_sense_dict[cui]

            try:
                if tty.endswith('AB'):
                    sense.add_abbr(text)
                else:
                    sense.add_longform(text)

                umls_sense_dict[cui] = sense
            except Exception as e:
                print(cui)
                print(tty)
                print(text)
                print(str(e)+'\n')

    return umls_sense_dict


def load_medline_abbr_dict():
    """
    Load the senses extracted from MEDLINE (by ADAM method)
    Each abbr corresponds to many longforms
     But each longform may appear in many abbr groups as well
    :return:
    abbr_long_dict, key is an abbr, value is a list of corresponding sense dicts
    long_abbr_dict, key is a longform, value is a list of corresponding sense dicts
    long_list, a list of all longforms
    """
    # create element tree object
    if os.path.exists(MEDLINE_sense_xml_path + '.cache'):
        with open(MEDLINE_sense_xml_path + '.cache', 'rb') as sense_file:
            abbr_senses_dict, longform_senses_dict, longform_list = pickle.load(sense_file)
    else:
        tree = xml.etree.ElementTree.parse(MEDLINE_sense_xml_path)

        # get root element
        root = tree.getroot()

        # create empty list for news items
        abbr_senses_dict = {}
        longform_senses_dict = {}
        longform_list = []

        # iterate news items
        for abbr in root:
            short_form = abbr.attrib['name']

            # iterate child groups under current abbr, each group consists a common longform and a list of alternative longforms
            for group in abbr:
                medline_sense_dict = {}
                medline_sense_dict['abbr'] = short_form

                medline_sense_dict['id'] = group.attrib['id'].strip()
                medline_sense_dict['total_freq'] = int(group.attrib['freq'].strip())

                # print('\t%s' % sense.attrib['normalized_name'])
                common_name = group.attrib['normalized_name'].strip()
                medline_sense_dict['common_name'] = common_name
                medline_sense_dict['longform_list'] = []
                medline_sense_dict['freq_list'] = []

                tmp_longform_list = []
                for alternative in group:
                    # print('\t\t%s' % alternative.text)
                    alter_longform = alternative.text.strip()
                    alter_freq = int(alternative.attrib['freq'])
                    medline_sense_dict['longform_list'].append(alter_longform)
                    medline_sense_dict['freq_list'].append(alter_freq)
                    tmp_longform_list.append(alter_longform)

                current_abbr_sense_list = abbr_senses_dict.get(short_form, [])
                current_abbr_sense_list.append(medline_sense_dict)
                abbr_senses_dict[short_form] = current_abbr_sense_list

                # iterate each longform and curate the longform_sense_dict
                for longform in tmp_longform_list:
                    longform_list.append(longform)
                    current_longform_sense_list = longform_senses_dict.get(longform, [])
                    current_longform_sense_list.append(medline_sense_dict)
                    longform_senses_dict[longform] = current_longform_sense_list

        print('\nMEDLINE DATA STATS')
        print('#(abbr_sense_dict) = %d' % len(abbr_senses_dict))
        print('#(longform_sense_dict) = %d' % len(longform_senses_dict))
        print('#(senses) = %d' % sum([len(s) for s in longform_senses_dict.values()]))
        print('#(longform_list) = %d' % len(longform_list))

        with open(MEDLINE_sense_xml_path + '.cache', 'wb') as sense_file:
            pickle.dump([abbr_senses_dict, longform_senses_dict, longform_list], sense_file)

    return abbr_senses_dict, longform_senses_dict, longform_list


def build_sense_inventory(umls_sense_inventory_dict,
                          medline_longform_sense_dict,
                          testset_abbr_cui_dict,
                          testset_cui_set):
    '''
    We load sense inventory from UMLS and MEDLINE respectively
    and then return the senses that appear in both sets
    :return:
    '''
    print('Stats of UMLS sense inventory (before merging MEDLINE)')
    print('#(sense) = %d' % len(umls_sense_inventory_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in umls_sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in umls_sense_inventory_dict.values()]))

    # reverse testset_abbr_cui_dict to get a cui_abbr_dict
    testset_cui_abbr_dict = {}
    for abbr, cui_set in testset_abbr_cui_dict.items():
        for cui in cui_set:
            abbr_set = testset_cui_abbr_dict.get(cui, set())
            abbr_set.add(abbr)
            testset_cui_abbr_dict[cui] = abbr_set

    # only keep the senses that appear in MEDLINE (related to medical and clinical)
    umls_and_medline_sense_inventory_dict = {}
    testcui_not_in_MEDLINE_count = 0

    for sense_id, umls_sense in umls_sense_inventory_dict.items():
        # iterate each longform appears in UMLS
        umls_longforms = [d['STR'] for d in umls_sense.original_umls_dicts]
        for umls_longform in umls_longforms:
            # check if it appears in MEDLINE
            if umls_longform in medline_longform_sense_dict:
                # add corresponding MEDLINE sense information to current UMLS sense
                umls_sense.original_medline_dicts.extend(medline_longform_sense_dict[umls_longform])
                umls_and_medline_sense_inventory_dict[umls_sense.cui] = umls_sense

        if umls_sense.cui in testset_cui_set and umls_sense.cui not in umls_and_medline_sense_inventory_dict:
            testcui_not_in_MEDLINE_count += 1

        # any sense that appears in testset is considered valid
        if umls_sense.cui in testset_cui_set:
            for abbr in testset_cui_abbr_dict[umls_sense.cui]:
                umls_sense.add_abbr(abbr)
            umls_and_medline_sense_inventory_dict[umls_sense.cui] = umls_sense


    print('\nStats of sense inventory (merging UMLS and MEDLINE)')
    print('#(sense) = %d' % len(umls_sense_inventory_dict))
    print('#(sense in testset but not in MEDLINE) = %d' % testcui_not_in_MEDLINE_count)
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in umls_sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in umls_sense_inventory_dict.values()]))

    return umls_and_medline_sense_inventory_dict


def dump_unfiltered_UMLS_and_MEDLINE_sense_inventory_to_disk(umls_and_medline_sense_dict):
    with open(UMLS_AND_MEDLINE_sense_inventory_path, 'wb') as sense_file:
        pickle.dump(umls_and_medline_sense_dict, sense_file)


def load_unfiltered_UMLS_and_MEDLINE_sense_inventory_from_disk():
    with open(UMLS_AND_MEDLINE_sense_inventory_path, 'rb') as sense_file:
        umls_and_medline_sense_dict = pickle.load(sense_file)

    print('\nStats of valid sense inventory (appear both in UMLS and MEDLINE)')
    print('#(sense) = %d' % len(umls_and_medline_sense_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in umls_and_medline_sense_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in umls_and_medline_sense_dict.values()]))

    return umls_and_medline_sense_dict


def merge_UMLS_and_MEDLINE_senses(sense_inventory_dict):
    '''
    Fill up the attributes of each sense from original UMLS/MEDLINE information
    :param sense_inventory_dict:
    :return:
    '''
    for cui, sense in sense_inventory_dict.items():

        for umls_sense in sense.original_umls_dicts:
            text = umls_sense['STR']
            if str(umls_sense['TTY']).endswith('AB'):
                sense.add_abbr(text)
            else:
                sense.add_longform(text)

        # print('Before merging medline: \n%s' % str(sense))

        for medline_sense in sense.original_medline_dicts:
            if 'id' in medline_sense:
                sense.add_medline_id(medline_sense['id'])
            sense.add_abbr(medline_sense['abbr'])
            sense.add_common_name(medline_sense['common_name'])
            # for longform in medline_sense['longform_list']:
            #     sense.add_longform(longform)

        # print('After merging medline: \n%s' % str(sense))
        # print('*' * 30)

    print('\nStats of sense inventory (after merging UMLS and MEDLINE senses)')
    print('#(sense) = %d' % len(sense_inventory_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in sense_inventory_dict.values()]))

    return sense_inventory_dict


def filter_by_cleaned_CUI_for_annotation(sense_inventory_dict, testset_cui_set):
    """
    The senses for annotation has been cleaned once, therefore we use its CUIs to filter our senses
    """
    cleaned_sense_for_annotation_df = pd.read_csv(CLEANED_SENSE_inventory_csv_path)
    cui_to_keep = set(cleaned_sense_for_annotation_df['id'].tolist()) | testset_cui_set

    filtered_sense_inventory_dict = {}
    for k,v in sense_inventory_dict.items():
        if k in cui_to_keep:
            filtered_sense_inventory_dict[k] = v

    print('\nStats of sense inventory (after filtering by the senses for annotation)')
    print('#(sense) = %d' % len(filtered_sense_inventory_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in filtered_sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in filtered_sense_inventory_dict.values()]))

    return filtered_sense_inventory_dict


def clean_abbrs(sense_inventory_dict):
    """
    Many abbrs are not valid (longform)
    :param sense_dict:
    :return:
    """
    false_abbr_list = []

    has_abbr_count = 0
    filtered_sense_inventory_dict = {}

    for cui, sense in sense_inventory_dict.items():
        new_abbr_set = set()
        for abbr in sense.abbr_set:
            abbr = str(abbr)
            if len(re.split(r'[\s\[\]\{\}]+', abbr)) > 1 \
                    and abbr not in ABBR_WHITELIST \
                    and not abbr.lower().endswith(' nos')\
                    and not abbr.lower().startswith('moab '):
                false_abbr_list.append(abbr)
                continue

            if len(abbr.strip()) == 0:
                false_abbr_list.append(abbr)
                continue

            if len(abbr) > 7 \
                    and abbr not in ABBR_WHITELIST \
                    and not abbr.lower().endswith(' nos')\
                    and not abbr.lower().startswith('moab ')\
                    and len(re.split(r'\W+', abbr)) == 1:
                false_abbr_list.append(abbr)
                continue

            if len(abbr.strip()) > 0:
                new_abbr_set.add(abbr.strip())

        # don't filter it now
        if len(new_abbr_set) > 0:
            sense.abbr_set = new_abbr_set
            filtered_sense_inventory_dict[cui] = sense
            has_abbr_count += 1

    # false_abbr_list = sorted(false_abbr_list, key=lambda x: len(x))
    # for abbr in false_abbr_list:
    #     print("'%s', " % abbr)

    print('#(senses w/ abbr)/#(all senses) = %d/%d' % (has_abbr_count, len(sense_inventory_dict)))

    return filtered_sense_inventory_dict


def clean_longforms(sense_inventory_dict):
    """
    some longforms are duplicates of abbr, remove them
    some longforms have irrelevant contents, such as (substance), remove these contents
    :param sense_inventory_dict:
    :return:
    """
    invalid_count = 0
    suspect_longforms = []

    for sense in sense_inventory_dict.values():
        lower_abbr_set = set([abbr.lower() for abbr in sense.abbr_set])
        new_longform_set = set()

        for longform in sense.longform_set:
            new_longform = str(longform)
            while re.search(r'^\[.*?\]| \(.*?\)$| \[.*?\]$| \{.*?\}$|<.*?>', new_longform):
                new_longform = re.sub(r'^\[.*?\]| \(.*?\)$| \[.*?\]$| \{.*?\}$|<.*?>','', new_longform).strip()
                # print(longform+ '\t->\t' + new_longform)

            if new_longform.lower().strip() in lower_abbr_set:
                # print('abbr-like longform: %s' % new_longform)
                continue

            if len(new_longform) > 0:
                new_longform_set.add(new_longform)

        if len(new_longform_set) > 0:
            sense.longform_set = new_longform_set
        else:
            print('No longforms left')
            print(new_longform_set)
            print(sense)

        lower_abbr_set = set([abbr.lower() for abbr in sense.abbr_set])

        for longform in sense.longform_set:
            new_longform = longform

            if len(new_longform) <= 4 and new_longform.find(' ') == -1 and not any(char.isdigit() for char in new_longform):
                suspect_longforms.append(new_longform)
                # for umls_sense in sense.original_umls_dicts:
                #     if new_longform == umls_sense['STR']:
                #         print(new_longform)
                #         print(umls_sense)
                #         print(sense)
                #         print('\n')


    print('\nStats of sense inventory (after clearning abbr and longform)')
    print('#(sense) = %d' % len(sense_inventory_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in sense_inventory_dict.values()]))


    max_len = max([len(l) for l in suspect_longforms])

    for i in range(1, max_len + 1):
        upper_longforms_at_length_i = sorted([l for l in suspect_longforms if len(l)==i])
        print('len=%d, found %d' % (i, len(upper_longforms_at_length_i)))
        print("'%s'" % ("', '".join(upper_longforms_at_length_i)))


    return sense_inventory_dict


def clean_senses_by_blacklist_and_common_words(sense_inventory_dict):
    """
    filter sense by checking whether the common name of a sense is a common words (google 20k) or not
    set a very loose rule to avoid false positive
        1. only one common name and it's one-word, most one-word senses are trivial
        2. number of longforms is equal or greater than 6, usually a complex concept
    :param sense_inventory_dict:
    :return:
    """
    COMMON_WORDS_VOCAB_set = set()
    with open(GOOGLE_20K_COMMON_WORDS_VOCAB_PATH, 'r') as google_words:
        for w in google_words:
            COMMON_WORDS_VOCAB_set.add(w.strip().lower())

    new_sense_inventory_dict = {}
    invalid_count = 0

    for cui, sense in sense_inventory_dict.items():
        common_name_valid_flag = True
        cui_valid_flag = True

        if cui in CUI_BLACKLIST:
            cui_valid_flag = False

        for common_name in sense.common_name_set:
            tokenized_common_name = common_name.lower().split()
            if len(tokenized_common_name) == 1 \
                    and common_name in COMMON_WORDS_VOCAB_set:
                print(sense)
                if len(sense.common_name_set) == 1 and len(sense.longform_set) < 6: # and len(tokenized_common_name[0]) > 3
                    print('FIND INVALID!\n')
                    common_name_valid_flag = False
                # print('-' * 30)

        if cui_valid_flag and common_name_valid_flag:
            new_sense_inventory_dict[cui] = sense
        else:
            invalid_count += 1


    print('\nStats of sense inventory (after cleaning invalid senses)')
    print('#(sense) = %d' % len(new_sense_inventory_dict))
    print('#(abbr) = %d' % np.sum([len(s.abbr_set) for s in new_sense_inventory_dict.values()]))
    print('#(long form) = %d' % np.sum([len(s.longform_set) for s in new_sense_inventory_dict.values()]))

    print('#(invalid sense) = %d' % invalid_count)

    return new_sense_inventory_dict


def output_to_pickle_and_json(sense_inventory_dict):
    with open(FINAL_CLEANED_SENSE_INVENTORY_JSON_PATH, 'w') as sense_json:
        for sense in sense_inventory_dict.values():
            sense_dict = sense.to_dict()
            sense_json.write(json.dumps(sense_dict) + '\n')

    with open(FINAL_CLEANED_SENSE_INVENTORY_PICKLE_PATH, 'wb') as sense_pickle:
        pickle.dump(sense_inventory_dict, sense_pickle)


def load_final_sense_inventory(path = FINAL_CLEANED_SENSE_INVENTORY_PICKLE_PATH):
    """
    Load the final sense inventory from disk, other than that, we also return a dict that use longform as key
    :return:
    """
    with open(path, 'rb') as sense_pickle:
        sense_inventory_dict = pickle.load(sense_pickle)

    long_sense_dict = {}
    for sense in sense_inventory_dict.values():
        for longform in sense.longform_set:
            # check if a longform appears in multiple senses
            if longform in long_sense_dict:
                print('Conflicted sense: %s' % long_sense_dict[longform])
                print('Current sense: %s' % sense)
                print('-' * 30)
            long_sense_dict[longform] = sense

    return sense_inventory_dict, long_sense_dict


def add_longform_abbr_mappings(sense_inventory_dict):
    '''
    Add the mappings of (longform, shortform) for each sense
    :param sense_inventory_dict:
    :return:
    '''
    col_names = ['CUI', 'LONGFORM', 'ABBR']
    annotated_longform_abbr_df = pd.read_csv(LONGFORM_ABBR_ANNOTATED_MAPPING_PATH, sep=',', header=None, names=col_names)
    grouped_umls_df = annotated_longform_abbr_df.groupby('CUI')

    pair_count = 0
    for group_id, (cui, group) in enumerate(grouped_umls_df):
        for row_id, row in group.iterrows():
            if not hasattr(sense_inventory_dict[cui], 'longform_abbr_mappings'):
                setattr(sense_inventory_dict[cui], 'longform_abbr_mappings', [])
            sense_inventory_dict[cui].longform_abbr_mappings.append({'LONGFORM': row['LONGFORM'], 'ABBR': row['ABBR']})
            pair_count += 1

    print('\nFound %d CUI with pairs' % len(grouped_umls_df))
    print('Found %d annotated pairs' % pair_count)
    print('Found %d senses without valid pairs' %
          len(list(filter(lambda s: not hasattr(sense_inventory_dict[cui], 'longform_abbr_mappings'
                               or len(s.longform_abbr_mappings) == 0),
                     sense_inventory_dict.values()))))

    # for cui, sense in sense_inventory_dict.items():
    #     if not hasattr(sense_inventory_dict[cui], 'longform_abbr_mappings') or len(sense.longform_abbr_mappings) == 0:
    #         print('Found no abbr-longform mapping')
    #         print(sense)
    #         print('\n')

    return sense_inventory_dict


def load_sense_inventories_from_testsets(umls_sense_inventory_dict=None, umls_longform_cui_dict=None):
    testset_cui_file = os.path.join(TEST_DATASET_BAES_PATH, 'testset_cui_existing_in_umls.pickle')
    if os.path.exists(testset_cui_file):
        with open(testset_cui_file, 'rb') as sense_pickle_file:
            join_abbr_cui_dict, join_cui_set = pickle.load(sense_pickle_file)
        return join_abbr_cui_dict, join_cui_set

    testset_abbr_cui_dict = {}
    testset_cui_set = set()

    testsets_with_cui = ['msh', 'share']
    testsets_without_cui = ['umn']

    for testset_name in testsets_with_cui:
        print('*' * 10 + testset_name.upper() + '*' * 10)
        cui_not_found_count = 0

        sense_inventory_file = os.path.join(TEST_DATASET_BAES_PATH, testset_name, 'sense_inventory.json')
        print(sense_inventory_file)
        testset_sense_inventory_dict = json.load(open(sense_inventory_file, 'r'))
        for abbr, cui_list in testset_sense_inventory_dict.items():
            for cui in cui_list:
                abbr_cui_set = testset_abbr_cui_dict.get(abbr.strip(), set())
                abbr_cui_set.add(cui.strip())
                testset_abbr_cui_dict[abbr.strip()] = abbr_cui_set

                testset_cui_set.add(cui.strip())
                if cui.strip() not in umls_sense_inventory_dict:
                    print(abbr)
                    print(cui)
                    cui_not_found_count += 1

        print('%d/%d concepts not found in UMLS' % (cui_not_found_count, len(testset_sense_inventory_dict)))

    for testset_name in testsets_without_cui:
        print('*' * 10 + testset_name.upper() + '*' * 10)
        cui_count = 0
        cui_not_found_count = 0
        longform_count = 0
        longform_not_found_count = 0

        sense_inventory_file = os.path.join(TEST_DATASET_BAES_PATH, testset_name, 'sense_cui_inventory.json')
        print(sense_inventory_file)

        testset_sense_inventory_dict = json.load(open(sense_inventory_file, 'r'))
        for abbr, longform_cui_dict in testset_sense_inventory_dict.items():
            for longform, cui_list in longform_cui_dict.items():
                longform_count += 1

                if cui_list is None:
                    cui_list = []
                    if longform.strip() in umls_longform_cui_dict:
                        cui_list = list(umls_longform_cui_dict[longform.strip()])
                    else:
                        longform_not_found_count += 1

                else:
                    cui_list = [c.strip() for c in cui_list.split(';')]

                for cui in cui_list:
                    abbr_cui_set = testset_abbr_cui_dict.get(abbr.strip(), set())
                    abbr_cui_set.add(cui.strip())
                    testset_abbr_cui_dict[abbr.strip()] = abbr_cui_set

                    testset_cui_set.add(cui.strip())
                    cui_count += 1

                    if cui not in umls_sense_inventory_dict:
                        print(abbr)
                        print(longform)
                        print(cui)
                        cui_not_found_count += 1

        print('%d/%d CUIs not found in UMLS' % (cui_not_found_count, cui_count))
        print('%d/%d longforms not found in UMLS' % (longform_not_found_count, longform_count))

    existing_sense_set = set(umls_sense_inventory_dict.keys())
    join_cui_set = testset_cui_set & existing_sense_set

    join_abbr_cui_dict = {}
    join_cui_set_for_validation = set()
    for abbr, cui_set in testset_abbr_cui_dict.items():
        # filter out the invalid CUIs
        abbr_join_cui_set = cui_set & join_cui_set
        join_abbr_cui_dict[abbr] = abbr_join_cui_set
        join_cui_set_for_validation = join_cui_set_for_validation | abbr_join_cui_set

    print('Found %d CUI in test datasets' % len(testset_cui_set))
    print('Found %d CUI in test dataset also appear in sense inventory' % len(join_cui_set))
    print('Found %d CUI in test dataset dict also appear in sense inventory' % len(join_cui_set_for_validation))

    testset_cui_file = os.path.join(TEST_DATASET_BAES_PATH, 'testset_cui_all.json')
    with open(testset_cui_file, 'w') as sense_json_file:
        json.dump(list(testset_cui_set), sense_json_file)

    testset_cui_file = os.path.join(TEST_DATASET_BAES_PATH, 'testset_cui_existing_in_umls.json')
    with open(testset_cui_file, 'w') as sense_json_file:
        json.dump(list(join_cui_set), sense_json_file)

    testset_cui_file = os.path.join(TEST_DATASET_BAES_PATH, 'testset_cui_existing_in_umls.pickle')
    with open(testset_cui_file, 'wb') as sense_pickle_file:
        pickle.dump([join_abbr_cui_dict, join_cui_set], sense_pickle_file)

    return join_abbr_cui_dict, join_cui_set


def set_is_in_testset(sense_inventory_dict, testset_cui_set):
    for cui in sense_inventory_dict.keys():
        sense = sense_inventory_dict[cui]
        if cui in testset_cui_set:
            setattr(sense, 'in_testset', True)
        else:
            setattr(sense, 'in_testset', False)

    return sense_inventory_dict

if __name__ == '__main__':
    """
    # load UMLS
    umls_sense_inventory_dict, umls_longform_cui_dict = load_umls_senses()
    # load MEDLINE
    _, medline_longform_sense_dict, _ = load_medline_abbr_dict()
    # load senses from testsets (only the CUIs that appear in UMLS)
    testset_abbr_cui_dict, testset_cui_set = load_sense_inventories_from_testsets(umls_sense_inventory_dict, umls_longform_cui_dict)

    # build sense inventory by merging UMLS ang MEDLINE
    sense_inventory_dict = build_sense_inventory(umls_sense_inventory_dict,
                                                 medline_longform_sense_dict,
                                                 testset_abbr_cui_dict,
                                                 testset_cui_set)
    sense_inventory_dict = merge_UMLS_and_MEDLINE_senses(sense_inventory_dict)
    dump_unfiltered_UMLS_and_MEDLINE_sense_inventory_to_disk(sense_inventory_dict)

    """
    testset_abbr_cui_dict, testset_cui_set = load_sense_inventories_from_testsets()
    # filter invalid senses through multiple steps
    sense_inventory_dict = load_unfiltered_UMLS_and_MEDLINE_sense_inventory_from_disk()

    existing_sense_set = set(sense_inventory_dict.keys())
    join_cui_set = testset_cui_set & existing_sense_set
    print('Found %d CUI in test datasets' % len(testset_cui_set))
    print('Found %d CUI in test datasets appear in sense inventory' % len(join_cui_set))

    sense_inventory_dict = filter_by_cleaned_CUI_for_annotation(sense_inventory_dict, testset_cui_set)

    existing_sense_set = set(sense_inventory_dict.keys())
    join_cui_set = testset_cui_set & existing_sense_set
    print('Found %d CUI in test datasets' % len(testset_cui_set))
    print('Found %d CUI in test datasets appear in sense inventory' % len(join_cui_set))

    # sense_inventory_dict = clean_senses_by_blacklist_and_common_words(sense_inventory_dict)
    sense_inventory_dict = clean_abbrs(sense_inventory_dict)
    sense_inventory_dict = clean_longforms(sense_inventory_dict)
    sense_inventory_dict = add_longform_abbr_mappings(sense_inventory_dict)

    existing_sense_set = set(sense_inventory_dict.keys())
    join_cui_set = testset_cui_set & existing_sense_set
    print('Found %d CUI in test datasets' % len(testset_cui_set))
    print('Found %d CUI in test datasets appear in sense inventory' % len(join_cui_set))

    sense_inventory_dict = set_is_in_testset(sense_inventory_dict, testset_cui_set)

    output_to_pickle_and_json(sense_inventory_dict)
